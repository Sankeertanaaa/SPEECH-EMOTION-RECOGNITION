import torch
import librosa
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification

# Choose device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Use a model that is actually trained for emotion recognition
model_name = "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name).to(device)

def predict_emotion(audio_path):
    # Load audio
    y, sr = librosa.load(audio_path, sr=16000)
    # Preprocess
    inputs = processor(y, sampling_rate=sr, return_tensors="pt", padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    # Predict
    with torch.no_grad():
        logits = model(**inputs).logits
    # For multi-label, use softmax; for single-label, use argmax
    probs = torch.softmax(logits, dim=-1).squeeze().cpu().numpy()
    label_id = probs.argmax()
    label = model.config.id2label[label_id]
    return label, probs[label_id]

# Example usage
if __name__ == "__main__":
    audio_file = "path/to/your/audio.wav"
    label, confidence = predict_emotion(audio_file)
    print(f"Predicted emotion: {label} (confidence: {confidence:.2f})")
